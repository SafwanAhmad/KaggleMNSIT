{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "\n",
    "dataframe = genfromtxt('train.csv', delimiter=',', skip_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, X = dataframe[:,0], dataframe[:, 1:]\n",
    "y = y.reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split it into train and validation\n",
    "n_train = int(0.7 * len(y))\n",
    "\n",
    "y_train, X_train, y_test, X_test = y[: n_train], X[: n_train, :], y[n_train:], X[n_train:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_26:0\", shape=(?, 784), dtype=float32)\n",
      "Tensor(\"Placeholder_27:0\", shape=(?, 1), dtype=int32)\n",
      "<tf.Variable 'Variable_106:0' shape=(784, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_107:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_108:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_109:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_110:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_111:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_112:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_113:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_114:0' shape=(128, 10) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_115:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_ = tf.placeholder(dtype=tf.float32, shape=(None, X_train.shape[1]))\n",
    "labels = tf.placeholder(dtype=tf.int32, shape=(None, 1))\n",
    "\n",
    "\n",
    "weight_input_hidden = tf.Variable(tf.random_normal(shape=(int(input_.shape[1]), 128), mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "bias_input_hidden = tf.Variable(tf.zeros(weight_input_hidden.shape[1]))\n",
    "\n",
    "\n",
    "weight_hidden1_hidden2 = tf.Variable(tf.random_normal(shape=(int(weight_input_hidden.shape[1]), 128), mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "bias_hidden1_hidden2 = tf.Variable(tf.zeros(weight_hidden1_hidden2.shape[1]))\n",
    "\n",
    "weight_hidden2_hidden3 = tf.Variable(tf.random_normal(shape=(int(weight_hidden1_hidden2.shape[1]), 128), mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "bias_hidden2_hidden3 = tf.Variable(tf.zeros(weight_hidden2_hidden3.shape[1]))\n",
    "\n",
    "weight_hidden3_hidden4 = tf.Variable(tf.random_normal(shape=(int(weight_hidden2_hidden3.shape[1]), 128), mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "bias_hidden3_hidden4 = tf.Variable(tf.zeros(weight_hidden3_hidden4.shape[1]))\n",
    "\n",
    "weight_hidden_output = tf.Variable(tf.random_normal(shape=(128, 10), mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "bias_hidden_output = tf.Variable(tf.zeros(weight_hidden_output.shape[1]))\n",
    "\n",
    "print(input_)\n",
    "print(labels)\n",
    "print(weight_input_hidden)\n",
    "print(bias_input_hidden)\n",
    "print(weight_hidden1_hidden2)\n",
    "print(bias_hidden1_hidden2)\n",
    "print(weight_hidden2_hidden3)\n",
    "print(bias_hidden2_hidden3)\n",
    "print(weight_hidden3_hidden4)\n",
    "print(bias_hidden3_hidden4)\n",
    "print(weight_hidden_output)\n",
    "print(bias_hidden_output)\n",
    "\n",
    "weight_dict = {'layer0': weight_input_hidden, \n",
    "               'layer1': weight_hidden1_hidden2, \n",
    "               'layer2': weight_hidden2_hidden3,\n",
    "               'layer3': weight_hidden3_hidden4,\n",
    "               'layer4': weight_hidden_output}\n",
    "bias_dict = {'layer0': bias_input_hidden, \n",
    "             'layer1': bias_hidden1_hidden2, \n",
    "             'layer2': bias_hidden2_hidden3,\n",
    "             'layer3': bias_hidden3_hidden4,\n",
    "             'layer4': bias_hidden_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nn():\n",
    "    hidden_input = tf.add(tf.matmul(input_, weight_dict['layer0']), bias_dict['layer0'])\n",
    "    hidden_output = tf.nn.relu(hidden_input)\n",
    "    hidden_output1 = hidden_output\n",
    "    \n",
    "    hidden_input = tf.add(tf.matmul(hidden_output, weight_dict['layer1']), bias_dict['layer1'])\n",
    "    hidden_output = tf.nn.relu(hidden_input)\n",
    "    \n",
    "    hidden_input = tf.add(tf.matmul(hidden_output, weight_dict['layer2']), bias_dict['layer2'])\n",
    "    hidden_output = tf.nn.relu(hidden_input)\n",
    "    hidden_output3 = hidden_output\n",
    "    \n",
    "    # Find average of hidden1_hidden2 and hidden3_hidden4\n",
    "    avg_output = tf.div(tf.add(hidden_output1, hidden_output3), 2)\n",
    "    \n",
    "    hidden_input = tf.add(tf.matmul(avg_output, weight_dict['layer3']), bias_dict['layer3'])\n",
    "    hidden_output = tf.nn.relu(hidden_input)\n",
    "    \n",
    "    output = tf.add(tf.matmul(hidden_output, weight_dict['layer4']), bias_dict['layer4'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the cost\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf.one_hot(labels,depth=10)))\n",
    "    \n",
    "    # Apply an optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.003).minimize(cost)\n",
    "    \n",
    "    return cost, optimizer, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch(batch_size, X, y):\n",
    "    for i in range(int(y.shape[0]/batch_size)-1):\n",
    "        yield X[i: i+batch_size, :] , y[i: i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost, optimizer, output = nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.064264335\n",
      "Train Accuracy: 0.786215400315\n",
      "Validation Accuracy: 0.779761904762\n",
      "\n",
      "Epoch: 0002 cost= 0.035573192\n",
      "Train Accuracy: 0.8427654237\n",
      "Validation Accuracy: 0.83962585034\n",
      "\n",
      "Epoch: 0003 cost= 0.041084133\n",
      "Train Accuracy: 0.843955950508\n",
      "Validation Accuracy: 0.837074829932\n",
      "\n",
      "Epoch: 0004 cost= 0.017243594\n",
      "Train Accuracy: 0.860028062418\n",
      "Validation Accuracy: 0.855952380952\n",
      "\n",
      "Epoch: 0005 cost= 0.017558625\n",
      "Train Accuracy: 0.835069518262\n",
      "Validation Accuracy: 0.840646258503\n",
      "\n",
      "Epoch: 0006 cost= 0.036153842\n",
      "Train Accuracy: 0.859177686126\n",
      "Validation Accuracy: 0.857993197279\n",
      "\n",
      "Epoch: 0007 cost= 0.016916748\n",
      "Train Accuracy: 0.870445171989\n",
      "Validation Accuracy: 0.874659863946\n",
      "\n",
      "Epoch: 0008 cost= 0.007872002\n",
      "Train Accuracy: 0.895913941919\n",
      "Validation Accuracy: 0.885884353741\n",
      "\n",
      "Epoch: 0009 cost= 0.013904958\n",
      "Train Accuracy: 0.889918789064\n",
      "Validation Accuracy: 0.893197278912\n",
      "\n",
      "Epoch: 0010 cost= 0.011884574\n",
      "Train Accuracy: 0.887920404779\n",
      "Validation Accuracy: 0.890816326531\n",
      "\n",
      "Epoch: 0011 cost= 0.012791876\n",
      "Train Accuracy: 0.899995748119\n",
      "Validation Accuracy: 0.89880952381\n",
      "\n",
      "Epoch: 0012 cost= 0.007518115\n",
      "Train Accuracy: 0.901228793741\n",
      "Validation Accuracy: 0.905612244898\n",
      "\n",
      "Epoch: 0013 cost= 0.031284809\n",
      "Train Accuracy: 0.904162591947\n",
      "Validation Accuracy: 0.907993197279\n",
      "\n",
      "Epoch: 0014 cost= 0.007647061\n",
      "Train Accuracy: 0.91032782006\n",
      "Validation Accuracy: 0.910204081633\n",
      "\n",
      "Epoch: 0015 cost= 0.004898221\n",
      "Train Accuracy: 0.905438156384\n",
      "Validation Accuracy: 0.90306122449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize tensorflow session\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "num_batches = int(X_train.shape[0]/batch_size)\n",
    "n_train = int(len(y_train)*0.8)\n",
    "\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #plt.ion()\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data for cross validation\n",
    "        indices = np.array(range(len(y_train)))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        train_indices = indices[: n_train]\n",
    "        val_indices = indices[n_train:]\n",
    "        \n",
    "        y_train_fold, X_train_fold, y_val_fold, X_val_fold = y_train[train_indices], X_train[train_indices, :], \\\n",
    "        y_train[val_indices], X_train[val_indices, :]\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for x,y in get_next_batch(batch_size, X_train_fold, y_train_fold):\n",
    "            sess.run(optimizer, feed_dict={input_:x, labels:y})\n",
    "            \n",
    "        c = sess.run(cost, feed_dict={input_: x, labels: y})\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))\n",
    "        \n",
    "        # Find train accuracy\n",
    "        y_predicted_train_fold = np.argmax(output.eval(feed_dict={input_: X_train_fold, labels: y_train_fold}), 1)\n",
    "        current_train_acc = accuracy_score(y_true=y_train_fold, y_pred=y_predicted_train_fold)\n",
    "        \n",
    "        train_accuracy.append(current_train_acc)\n",
    "        \n",
    "        print(\"Train Accuracy:\", current_train_acc)\n",
    "        \n",
    "        # Find the validation accuracy\n",
    "        y_predicted_val_fold = np.argmax(output.eval(feed_dict={input_: X_val_fold, labels: y_val_fold}), 1)\n",
    "        current_val_acc = accuracy_score(y_true=y_val_fold, y_pred=y_predicted_val_fold)\n",
    "        \n",
    "        val_accuracy.append(current_val_acc)\n",
    "        \n",
    "        print(\"Validation Accuracy:\", current_val_acc)\n",
    "        print()\n",
    "        \n",
    "        #plt.plot(train_accuracy, 'b-', val_accuracy, 'r-')\n",
    "        #plt.show()\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "    # Find test accuracy\n",
    "    y_predicted_test = np.argmax(output.eval(feed_dict={input_: X_test, labels: y_test}), 1)\n",
    "    print(\"Test Accuracy:\", accuracy_score(y_true=y_test, y_pred=y_predicted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_accuracy, 'b-', val_accuracy, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_true=y_test, y_pred=y_predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = np.where(np.logical_and(y_predicted_test == 9, y_test.squeeze() == 4))[0]\n",
    "for i in indices:\n",
    "    plt.figure(figsize=(0.5,0.5))\n",
    "    plt.imshow(X_test[i].reshape([28,28]),cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
